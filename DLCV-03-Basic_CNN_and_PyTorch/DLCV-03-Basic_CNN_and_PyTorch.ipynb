{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed707c8",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f712",
   "metadata": {},
   "source": [
    "In computer vision, a convolutional neural network(CNN) is a well-known deep learning technique that is widely used by the state-of-the-art(SOTA) vision models such as AlexNet, ResNet and YOLO. CNNs are specifically designed for image related tasks such as classification, segmentation, object detection and 3D reconstruction.  \n",
    "\n",
    "In contrast to artificial neural network(ANN), convolutional layers use small filters(kernels) to extract the features from images. Each filter from a CNN consists of a set of trainable parameters that are shared in order to reduce the number of parameters.  Weight sharing in CNNs means that the same set of learnable parameters (weights and biases) is used for all spatial locations of the input when applying a filter. This design allows CNNs to efficiently learn and detect features throughout the input data while maintaining a manageable number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba1f56",
   "metadata": {},
   "source": [
    "<img src=\"img/cnn.gif\" alt=\"CNN GIF\" width=\"500px\" style=\"float: center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eacd1c",
   "metadata": {},
   "source": [
    "#### CNN Explainer\n",
    "\n",
    "Before doing this tutorial, take a look at the [CNN Explainer](https://poloclub.github.io/cnn-explainer/). It gives beautiful illustrations of what's happening in a CNN at every level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691994c",
   "metadata": {},
   "source": [
    "#### Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0b93a",
   "metadata": {},
   "source": [
    "In CNN, a kernel, also known as a filter, is a small matrix of learnable parameters that is used to perform a convolution operation on an input data volume, which is typically an image or a feature map from a previous layer. Kernels are one of the fundamental building blocks of CNNs and play a crucial role in feature extraction.  \n",
    "\n",
    "A few things to know about kernel:  \n",
    "1. **Kernel Size**  \n",
    "A kernel can be a square or a rectangular matrix with small dimension. The commonly used sizes are 3x3 or 5x5.  \n",
    "\n",
    "2. **Weights and Biases**  \n",
    "Each element within the kernel is a learnable parameter, represented by a weight. Additionally, there is usually one bias term associated with each kernel. These weights and biases are learned during the training process through backpropagation.  \n",
    "\n",
    "3. **Convolution Operation**  \n",
    "The kernel is applied to the input data by sliding it over the input in a systematic way. Specifically, the kernel is placed on the top-left corner of the input, and a mathematical operation called convolution is performed by taking the **element-wise product** of the kernel and the overlapping portion of the input. These products are then **summed** to produce a single value, which forms a pixel in the output feature map.  \n",
    "\n",
    "<img src=\"img/convolve.png\" alt=\"Convolution\" width=\"400px\" />\n",
    "\n",
    "4. **Feature Extraction**  \n",
    "The purpose of the kernel is to extract specific features from the input data. Each kernel is designed to capture a particular pattern or feature, such as edges, textures, or other local characteristics.  \n",
    "\n",
    "5. **Strides and Padding**  \n",
    "The convolution operation can have parameters like stride and padding. Stride determines how much the kernel is moved horizontally and vertically with each step, affecting the spatial dimensions of the output feature map. Padding can be used to control the size of the output feature map and ensure that the spatial dimensions remain the same as the input. \n",
    "\n",
    "<img src=\"img/stride.png\" alt=\"Stride\" width=\"400px\" />\n",
    "\n",
    "6. **Multiple Kernels**  \n",
    "In a single convolutional layer, multiple kernels are typically used. Each kernel captures a different feature, and together, they produce multiple feature maps as the layer's output. These feature maps collectively represent a set of learned features at different scales and orientations.  \n",
    "\n",
    "7. **Stacking Convolutional Layers**  \n",
    "CNNs often consist of multiple convolutional layers stacked together. These layers extract increasingly complex and abstract features as you move deeper into the network.  \n",
    "\n",
    "<img src=\"img/stackedconvolution.png\" alt=\"Stacked Convolution\" width=\"400px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c16b18f",
   "metadata": {},
   "source": [
    "#### Connecting to Google Drive for Colab Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abf762",
   "metadata": {},
   "source": [
    "For the students who use google colab for run the system. It is better to upload your dataset or weight, model into your drive. The colab system will be reset everytime when the system has log off.\n",
    "\n",
    "Ok, let's mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for colab only, please uncomment.\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # Your root path in gdrive\n",
    "# root_path = 'gdrive/your_directory_in_google_drive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c998999",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To create the folder\n",
    "import os\n",
    "\n",
    "\n",
    "# print(os.getcwd()) ### Uncomment to get your current working directory\n",
    "# root_path = 'your/root/path/in/local/machine' ### Uncomment and modify your root path if you are using own laptop\n",
    "lab_path =  root_path + 'lab03/'\n",
    "if not os.path.exists(lab_path):\n",
    "    print(\"No folder \", lab_path, 'exist. Creating a new folder...')\n",
    "    os.mkdir(lab_path)\n",
    "    print(\"New directory is created.\")\n",
    "else:\n",
    "    print(vdo_path, 'exists, do nothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a789ea",
   "metadata": {},
   "source": [
    "# Building CNN from Scratch using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f9d93",
   "metadata": {},
   "source": [
    "This implementation referenced from [Ahmed Gad's tutorial](https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html).  \n",
    "\n",
    "We will implement a very simple CNN in numpy. The model will have just three layers, a convolutional layer (conv for short), a ReLU activation layer, and max pooling. The major steps involved are as follows:  \n",
    "\n",
    "1. Reading the input image.\n",
    "2. Preparing filters.\n",
    "3. Conv layer: Convolving each filter with the input image.\n",
    "4. ReLU layer: Applying ReLU activation function on the feature maps (output of conv layer).\n",
    "5. Max Pooling layer: Applying the pooling operation on the output of ReLU layer.\n",
    "6. Stacking the conv, ReLU, and max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b65641",
   "metadata": {},
   "source": [
    "#### 1. Reading an Input Image\n",
    "\n",
    "The following code reads an existing image using the `scikit-image` Python library and converts it into grayscale. You may need to `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee99121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read image\n",
    "img = skimage.data.chelsea()\n",
    "print('Image dimensions:', img.shape)\n",
    "\n",
    "# Convert to grayscale\n",
    "img = skimage.color.rgb2gray(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0e9f9",
   "metadata": {},
   "source": [
    "#### 2. Preparing Filters\n",
    "\n",
    "Recall that a conv layer uses some number of convolution (actually cross correlation) filters, usually matching the number of channels in the input (1 in our case since the image is grayscale). Each kernel gives us one feature map (channel) in the result.\n",
    "\n",
    "Let's make two 3 &times; 3 filters, using the horizontal and vertical Sobel edge filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce17cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filters = np.zeros((2,3,3))\n",
    "l1_filters[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                 [-2, 0, 2], \n",
    "                                 [-1, 0, 1]]])\n",
    "l1_filters[1, :, :] = np.array([[[-1, -2, -1], \n",
    "                                 [ 0,  0,  0], \n",
    "                                 [ 1,  2,  1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad247bf0",
   "metadata": {},
   "source": [
    "#### 3. Conv Layer\n",
    "\n",
    "Let's perform the convolution operation to the input image with our filters.  \n",
    "\n",
    "Before convolving, one important thing to notice is the calculation of the size of convolution output. We can memorize as the following:  \n",
    "\n",
    "$$\\text{output size} = \\frac{\\mathit{n - f + 2p}}{\\mathit{s}}+1$$  \n",
    "\n",
    "Where,  \n",
    "$\\mathit{n}$ = input size (width or height)  \n",
    "$\\mathit{f}$ = filter size  \n",
    "$\\mathit{p}$ = padding  \n",
    "$\\mathit{s}$ = stride  \n",
    "\n",
    "P.S. The output size for each dimension have to be calculated separately if the input width and height are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stride 1 cross correlation of an image and a filter. We output the valid region only\n",
    "# (no padding).\n",
    "def convolve(img, conv_filter):\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    results_dim = ((np.array(img.shape) - np.array(conv_filter.shape) + (2*padding))/stride) + 1\n",
    "    result = np.zeros((int(results_dim[0]), int(results_dim[1])))\n",
    "    \n",
    "    for r in np.arange(0, img.shape[0] - filter_size + 1):\n",
    "        for c in np.arange(0, img.shape[1]-filter_size + 1):          \n",
    "            curr_region = img[r:r+filter_size,c:c+filter_size]\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result)\n",
    "            result[r, c] = conv_sum\n",
    "    \n",
    "    return result       \n",
    "\n",
    "\n",
    "# Perform convolution with a set of filters and return the result\n",
    "def conv(img, conv_filters):\n",
    "    # Check shape of inputs\n",
    "    if len(img.shape) != len(conv_filters.shape) - 1: \n",
    "        raise Exception(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "\n",
    "    # Ensure filter depth is equal to number of channels in input\n",
    "    if len(img.shape) > 2 or len(conv_filters.shape) > 3:\n",
    "        if img.shape[-1] != conv_filters.shape[-1]:\n",
    "            raise Exception(\"Error: Number of channels in both image and filter must match.\")\n",
    "            \n",
    "    # Ensure filters are square\n",
    "    if conv_filters.shape[1] != conv_filters.shape[2]: \n",
    "        raise Exception('Error: Filter must be square (number of rows and columns must match).')\n",
    "\n",
    "    # Ensure filter dimensions are odd\n",
    "    if conv_filters.shape[1]%2==0: \n",
    "        raise Exception('Error: Filter must have an odd size (number of rows and columns must be odd).')\n",
    "\n",
    "    # Prepare output\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filters.shape[1]+1, \n",
    "                             img.shape[1]-conv_filters.shape[1]+1, \n",
    "                             conv_filters.shape[0]))\n",
    "\n",
    "    # Perform convolutions\n",
    "    for filter_num in range(conv_filters.shape[0]):\n",
    "        curr_filter = conv_filters[filter_num, :]\n",
    "# Our convolve function only handles 2D convolutions. If the input has multiple channels, we\n",
    "# perform the 2D convolutions for each input channel separately then add them. If the input\n",
    "# has just a single channel, we do the convolution directly.\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = convolve(img[:, :, 0], curr_filter[:, :, 0])\n",
    "            for ch_num in range(1, curr_filter.shape[-1]):\n",
    "                conv_map = conv_map + convolve(img[:, :, ch_num], \n",
    "                                      curr_filter[:, :, ch_num])\n",
    "        else:\n",
    "            conv_map = convolve(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d804d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conv(img, l1_filters)\n",
    "%timeit conv(img,l1_filters)\n",
    "\n",
    "print('Convolutional feature maps shape:', features.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(features[:,:,0], cmap='gray')\n",
    "ax2.imshow(features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7e554",
   "metadata": {},
   "source": [
    "#### 4. ReLU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f07c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "\n",
    "def relu(feature_map):\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cefcf3",
   "metadata": {},
   "source": [
    "#### 5. Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer with particular size and stride\n",
    "\n",
    "def pooling(feature_map, size=2, stride=2):\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                         np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                         feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f7a42",
   "metadata": {},
   "source": [
    "Now let's try ReLU and pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relued_features = relu(features)\n",
    "pooled_features = pooling(relued_features)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "ax1.imshow(relued_features[:,:,0], cmap='gray')\n",
    "ax2.imshow(relued_features[:,:,1], cmap='gray')\n",
    "ax3.imshow(pooled_features[:,:,0], cmap='gray')\n",
    "ax4.imshow(pooled_features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4009b",
   "metadata": {},
   "source": [
    "#### 6. Stacking Conv, ReLU and Maxpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616b374",
   "metadata": {},
   "source": [
    "We will continue stacking the conv, relu, maxpool sequence into layer by layer until we get the 3 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First conv layer\n",
    "\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(\"conv layer 1...\")\n",
    "l1_feature_maps = conv(img, l1_filters)\n",
    "l1_feature_maps_relu = relu(l1_feature_maps)\n",
    "l1_feature_maps_relu_pool = pooling(l1_feature_maps_relu, 2, 2)\n",
    "\n",
    "# Second conv layer\n",
    "\n",
    "print(\"conv layer 2...\")\n",
    "l2_filters = np.random.rand(3, 5, 5, l1_feature_maps_relu_pool.shape[-1])\n",
    "l2_feature_maps = conv(l1_feature_maps_relu_pool, l2_filters)\n",
    "l2_feature_maps_relu = relu(l2_feature_maps)\n",
    "l2_feature_maps_relu_pool = pooling(l2_feature_maps_relu, 2, 2)\n",
    "#print(l2_feature_maps)\n",
    "\n",
    "# Third conv layer\n",
    "\n",
    "print(\"conv layer 3...\")\n",
    "l3_filters = np.random.rand(1, 7, 7, l2_feature_maps_relu_pool.shape[-1])\n",
    "l3_feature_maps = conv(l2_feature_maps_relu_pool, l3_filters)\n",
    "l3_feature_maps_relu = relu(l3_feature_maps)\n",
    "l3_feature_maps_relu_pool = pooling(l3_feature_maps_relu, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f9f25",
   "metadata": {},
   "source": [
    "We have completed feed forward processing of 3 (conv, relu, maxpool) layers. Now let's try to visualize each layer output feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "\n",
    "fig0, ax0 = plt.subplots(nrows=1, ncols=1)\n",
    "ax0.imshow(img).set_cmap(\"gray\")\n",
    "ax0.set_title(\"Input Image\")\n",
    "ax0.get_xaxis().set_ticks([])\n",
    "ax0.get_yaxis().set_ticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "fig1, ax1 = plt.subplots(nrows=3, ncols=2)\n",
    "fig1.set_figheight(10)\n",
    "fig1.set_figwidth(10)\n",
    "ax1[0, 0].imshow(l1_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[0, 0].get_xaxis().set_ticks([])\n",
    "ax1[0, 0].get_yaxis().set_ticks([])\n",
    "ax1[0, 0].set_title(\"L1-Map1\")\n",
    "\n",
    "ax1[0, 1].imshow(l1_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[0, 1].get_xaxis().set_ticks([])\n",
    "ax1[0, 1].get_yaxis().set_ticks([])\n",
    "ax1[0, 1].set_title(\"L1-Map2\")\n",
    "\n",
    "ax1[1, 0].imshow(l1_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[1, 0].get_xaxis().set_ticks([])\n",
    "ax1[1, 0].get_yaxis().set_ticks([])\n",
    "ax1[1, 0].set_title(\"L1-Map1ReLU\")\n",
    "\n",
    "ax1[1, 1].imshow(l1_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[1, 1].get_xaxis().set_ticks([])\n",
    "ax1[1, 1].get_yaxis().set_ticks([])\n",
    "ax1[1, 1].set_title(\"L1-Map2ReLU\")\n",
    "\n",
    "ax1[2, 0].imshow(l1_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 0].set_title(\"L1-Map1ReLUPool\")\n",
    "\n",
    "ax1[2, 1].imshow(l1_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 1].set_title(\"L1-Map2ReLUPool\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "fig2, ax2 = plt.subplots(nrows=3, ncols=3)\n",
    "fig2.set_figheight(12)\n",
    "fig2.set_figwidth(12)\n",
    "ax2[0, 0].imshow(l2_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[0, 0].get_xaxis().set_ticks([])\n",
    "ax2[0, 0].get_yaxis().set_ticks([])\n",
    "ax2[0, 0].set_title(\"L2-Map1\")\n",
    "\n",
    "ax2[0, 1].imshow(l2_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[0, 1].get_xaxis().set_ticks([])\n",
    "ax2[0, 1].get_yaxis().set_ticks([])\n",
    "ax2[0, 1].set_title(\"L2-Map2\")\n",
    "\n",
    "ax2[0, 2].imshow(l2_feature_maps[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[0, 2].get_xaxis().set_ticks([])\n",
    "ax2[0, 2].get_yaxis().set_ticks([])\n",
    "ax2[0, 2].set_title(\"L2-Map3\")\n",
    "\n",
    "ax2[1, 0].imshow(l2_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[1, 0].get_xaxis().set_ticks([])\n",
    "ax2[1, 0].get_yaxis().set_ticks([])\n",
    "ax2[1, 0].set_title(\"L2-Map1ReLU\")\n",
    "\n",
    "ax2[1, 1].imshow(l2_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[1, 1].get_xaxis().set_ticks([])\n",
    "ax2[1, 1].get_yaxis().set_ticks([])\n",
    "ax2[1, 1].set_title(\"L2-Map2ReLU\")\n",
    "\n",
    "ax2[1, 2].imshow(l2_feature_maps_relu[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[1, 2].get_xaxis().set_ticks([])\n",
    "ax2[1, 2].get_yaxis().set_ticks([])\n",
    "ax2[1, 2].set_title(\"L2-Map3ReLU\")\n",
    "\n",
    "ax2[2, 0].imshow(l2_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[2, 0].get_xaxis().set_ticks([])\n",
    "ax2[2, 0].get_yaxis().set_ticks([])\n",
    "ax2[2, 0].set_title(\"L2-Map1ReLUPool\")\n",
    "\n",
    "ax2[2, 1].imshow(l2_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[2, 1].get_xaxis().set_ticks([])\n",
    "ax2[2, 1].get_yaxis().set_ticks([])\n",
    "ax2[2, 1].set_title(\"L2-Map2ReLUPool\")\n",
    "\n",
    "ax2[2, 2].imshow(l2_feature_maps_relu_pool[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[2, 2].get_xaxis().set_ticks([])\n",
    "ax2[2, 2].get_yaxis().set_ticks([])\n",
    "ax2[2, 2].set_title(\"L2-Map3ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=3)\n",
    "fig3.set_figheight(15)\n",
    "fig3.set_figwidth(15)\n",
    "ax3[0].imshow(l3_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[0].get_xaxis().set_ticks([])\n",
    "ax3[0].get_yaxis().set_ticks([])\n",
    "ax3[0].set_title(\"L3-Map1\")\n",
    "\n",
    "ax3[1].imshow(l3_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[1].get_xaxis().set_ticks([])\n",
    "ax3[1].get_yaxis().set_ticks([])\n",
    "ax3[1].set_title(\"L3-Map1ReLU\")\n",
    "\n",
    "ax3[2].imshow(l3_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[2].get_xaxis().set_ticks([])\n",
    "ax3[2].get_yaxis().set_ticks([])\n",
    "ax3[2].set_title(\"L3-Map1ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47b6c8",
   "metadata": {},
   "source": [
    "We can see that at progressively higher layers of the network, we get coarser representations of the input. Since the filters at the later layers are random, they are not very structured, so we get a kind of blurring effect. These visualizations would be more meaningful in model with learned filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e4aff",
   "metadata": {},
   "source": [
    "# CNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af49253",
   "metadata": {},
   "source": [
    "Now we'll do a more complete CNN example using PyTorch. We'll use the MNIST digits again. The example is based on [Anand Saha's PyTorch tutorial](https://github.com/anandsaha/deep.learning.with.pytorch).  \n",
    "\n",
    "PyTorch has a few useful modules for us:\n",
    "\n",
    "1. cuda: `Compute Unified Device Architecture` a GPU-based tensor computations\n",
    "2. nn: Neural network layer implementations and backpropagation via autograd\n",
    "3. torchvision: datasets, models, and image transformations for computer vision problems.  \n",
    "\n",
    "torchvision itself includes several useful elements:  \n",
    "\n",
    "1. datasets: Datasets are subclasses of torch.utils.data.Dataset. Some of the common datasets available are \"MNIST,\" \"COCO,\" and \"CIFAR.\" In this example we will see how to load MNIST dataset using a custom subclass of the datasets class.\n",
    "2. transforms - Transforms are used for image transformations. The MNIST dataset from torchvision is in PIL image.  \n",
    "\n",
    "To convert MNIST images to tensors, we will use transforms.`ToTensor()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b3473",
   "metadata": {},
   "source": [
    "### Begin Your PyTorch\n",
    "\n",
    "First of all, import torch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa30546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# The functional module contains helper functions for defining neural network layers as simple functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d7ae3",
   "metadata": {},
   "source": [
    "### Important libraries\n",
    "\n",
    "- torch - main package of pytorch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix–vector multiplication, matrix–matrix multiplication and matrix product.\n",
    "- torchvision - part of pytorch. The torchvision package consists of\n",
    "    - popular datasets\n",
    "    - model architectures\n",
    "    - common image transformations for computer vision  \n",
    "    \n",
    "### Load the MNIST data\n",
    "\n",
    "Next, let's load the data and transfrom the input elements (pixels) using torchvision, and do transformation to torch type and normalize the data using their mean over the entire training dataset is 0 and its standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired mean and standard deviation\n",
    "mean = 0.0\n",
    "stddev = 1.0\n",
    "\n",
    "# Transform input image\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((mean), (stddev))])\n",
    "\n",
    "# Download dataset to ./data\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "# for colab user, use root_path\n",
    "# mnist_train = datasets.MNIST(root_path + 'data', train=True, download=True, transform=transform)\n",
    "# mnist_valid = datasets.MNIST(root_path + 'data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cc83d",
   "metadata": {},
   "source": [
    "Visualize sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a775fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mnist_train[12][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[12][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "label = mnist_train[100][1]\n",
    "print('Label of image above:', label)\n",
    "img = mnist_train[100][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221f80f",
   "metadata": {},
   "source": [
    "Create a `Dataloader` that loads the data and splits them into desired batches which are ready to feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cd499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size if you get out-of-memory error\n",
    "batch_size = 1024\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in mnist_train_loader:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6ff0",
   "metadata": {},
   "source": [
    "### Create a Basic CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b852f72",
   "metadata": {},
   "source": [
    "We use 2 convolutional layers followed by 2 fully connected layers. The input size of each image is (1,28,28). We will use stide of size 1 and padding of size 0. We called that `Valid` convolution. (Please also study on `Same` convolution)  \n",
    "\n",
    "For first convolution layer we will apply 10 filters of size (5,5). CNN output formula  \n",
    "\n",
    "$$\\text{output size} = \\frac{W - F + 2P}{S} + 1$$  \n",
    "where,  \n",
    "$W$ - input  \n",
    "$F$ - filter size  \n",
    "$P$ - padding size  \n",
    "$S$ - stride size  \n",
    "\n",
    "We get $\\frac{(1,28,28) - (1,5,5) + (2*0)}{1} + 1$ for each filter, so for 10 filters we get output size of (24,24,10). The ReLU activation function is applied to the output of the first convolutional layer.\n",
    "\n",
    "For the second convolutional layer, we apply 20 filters of size (5,5), giving us the output size of $\\frac{(10,24,24) - (10,5,5) + (2*0)}{1} + 1$ = (20,20,20). (20,20,20). Maxpooling with a size of 2 is applied to the output of the second convolutional layer, thereby giving us an output size of (10,10,20). Then, the ReLU activation function is applied to the output of the maxpooling layer.\n",
    "\n",
    "Next we have two fully connected layers. The input of the first fully connected layer is flattened output of $10*10*20 = 2000$, with 50 nodes. The fc second layer is the output layer and has 10 nodes.\n",
    "\n",
    "#### Important Terms (for PyTorch)\n",
    "\n",
    "**Tensor** - any matrix arrays which use for calculation, you can call input, output, weight as any tensors. In CNNs, we use \"input tensor\" as input; i.e. image, and \"output tensor\" as output.\n",
    "\n",
    "**Kernel** - filter tensor, or weight tensor. In computer vision, we might call mask tensor or mask matrix.\n",
    "\n",
    "**Channel** - number of depth in tensor, so sometime we call **depth** in 2D images. (Notice that the term **depth** is changed when the dimension of the image is 3D volumetric)\n",
    "\n",
    "**Feature** - Specific characteristic information for using in **dense layers** or **fully connect layers**.\n",
    "\n",
    "**Feature extraction** - the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set.\n",
    "\n",
    "**Stride** - The jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in.\n",
    "\n",
    "**Padding** - the zero array extends in both sides of tensor.\n",
    "\n",
    "\n",
    "In PyTorch, the function nn.Conv2d() does not require the `input size` argument, but it needs to fill number of channels and kernel size, including operation in the layer. For dense layer or fully connected layer or linear layer, we need to set input features number and output features number. Thus, it is necessary to understand how to calculate tensors and feature size in each layer output.  \n",
    "\n",
    "\n",
    "### PyTorch model architecture\n",
    "\n",
    "PyTorch deep learning models come in (at least) two possible styles:\n",
    "\n",
    "<img src=\"img/NNinPytorch.png\" style=\"width: 500px;\" />\n",
    "\n",
    "1. The PyTorch Sequential API is very expressive when we have a straightforward sequence of operations to perform on the input.\n",
    "2. The PyTorch Module allows more flexible transformations of inputs, combination of multiple inputs, generation of multiple outputs, and so on.\n",
    "\n",
    "### Number of parameters and output tensors size calculation\n",
    "\n",
    "#### CNN Parameters\n",
    "\n",
    "Kernel size $k$ in 1 layer for 1 channel output can be calculated by\n",
    "\n",
    "$$k = k_w \\times k_h \\times i_c$$\n",
    "\n",
    "where,  \n",
    "$k_w$ = width of kernel  \n",
    "$k_h$ = height of kernel  \n",
    "$i_c$ = input channels  \n",
    "\n",
    "We need to have $o_c$ kernels for release $o_c$ output channels. Therefore, for 1 layer of CNN, number of parameters can be calculated as\n",
    "\n",
    "$$n_p = k \\times o_c = (k_w \\times k_h \\times i_c) \\times o_c$$\n",
    "\n",
    "For bias in CNNs, it usually become all zeros, but we can assign bias CNNs in PyTorch. The **bias size** is equal to the **output tensor size**.\n",
    "\n",
    "#### Fully Connect Parameters\n",
    "\n",
    "Weight size $s_w$ in 1 layers can be calculated by\n",
    "\n",
    "$$s_w = i_f \\times o_f$$\n",
    "\n",
    "when $i_f$ is input features, and $o_f$ is output features. For bias, the size is equal to **output feature size**.\n",
    "\n",
    "We can calculate that the total parameters number is the parameters of all layers, so the network size can be used from the parameters number. It is useful to tell how efficient of the network (How fast)\n",
    "\n",
    "#### Output Tensors Size\n",
    "\n",
    "If we have an input tensor or image input size $w \\times h$ which want to convolution with $k_w \\times k_h$ kernel size with padding $p$ and stride $s$, we can calculate output tensor size as:\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor \\times \\lfloor \\frac{h+2p-k_h}{s} + 1 \\rfloor $$\n",
    "\n",
    "For example, input image in the first layer is $224 \\times 224$. Using $11 \\times 11$ of kernel size with padding $2$ and stride 4. We calculate\n",
    "\n",
    "$$output_{size}=\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor = \\lfloor \\frac{224+2(2)-11}{4} + 1 \\rfloor = \\lfloor 55.25 \\rfloor = 55$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
    "        self.relu1 = nn.ReLU()                            # Same as above\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)     # 20 x 20 x 20  (after 2nd convolution)\n",
    "        #self.conv2_drop = nn.Dropout2d(p=0.5)            # Dropout is a regularization technqiue we discussed in class\n",
    "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 20  (after pooling)\n",
    "        self.relu2 = nn.ReLU()                            # Same as above \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolution Layer 1                    \n",
    "        x = self.conv1(x)                        \n",
    "        x = self.relu1(x)                        \n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)               \n",
    "        #x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Switch from activation maps to vectors\n",
    "        x = x.view(-1, 2000)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51439a8",
   "metadata": {},
   "source": [
    "#### Create the Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "model = CNN_Model()\n",
    "\n",
    "if cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bd0c0",
   "metadata": {},
   "source": [
    "#### Print Out the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d097857",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0019db",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct.cpu() / float(len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = model(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6747cc",
   "metadata": {},
   "source": [
    "We can see that the model is still learning something. We might want to train another 10 epochs or so to see if validation accuracy increases further. For now, though, we'll just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"lab03/model_epoch20.pth\")\n",
    "# for colab user\n",
    "#torch.save(model.state_dict(), root_path + \"lab03/model_epoch20.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(valid_loss, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_accuracy, label='training accuracy')\n",
    "plt.plot(valid_accuracy, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08250c0c",
   "metadata": {},
   "source": [
    "What can you conclude from the loss and accuracy curves?\n",
    "1. We are not overfitting (at least not yet)\n",
    "2. We should continue training, as validation loss is still improving\n",
    "3. Validation accuracy is much higher than last week's fully connected models\n",
    "\n",
    "Now let's test on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91339225",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 9\n",
    "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
    "img = Variable(img)\n",
    "label = mnist_valid[image_index][1]\n",
    "plt.imshow(img[0,0])\n",
    "model.eval()\n",
    "\n",
    "if cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    model = model.cpu()\n",
    "    img = img.cpu()\n",
    "    \n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(\"Predicted label:\", predicted[0].item())\n",
    "print(\"Actual label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e9800",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fba0a3",
   "metadata": {},
   "source": [
    "Train and test CIFAR-10 dataset on a **New Network** with the following criteria.  \n",
    "\n",
    "1. The model should have at least 3 convolutional layers with `Same Convolutions` (We have done with `Valid Convolutions` in our exercise).\n",
    "2. Train on a GPU to have at least 95% **validation** accuracy and plot the loss & accuracy graphs. You have to somehow **tune the hyperparameters** in order to achieve such accuracy.\n",
    "3. While training, find the optimal fit value for the batch size in your gpu. Show your GPU usage using `nvidia-smi` command.\n",
    "4. Test and show the predicted images on **at least 10 images**.\n",
    "5. Explain your **training steps(including how you tune)** and **discuss about the result** briefly.\n",
    "\n",
    "**Hint**:  \n",
    "- the dataset can be download as below.\n",
    "- you may use data augmentation (optinal)\n",
    "- please use a research concept approach to explore and explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up preprocessing of CIFAR-10 images to 3x28x28 with normalization\n",
    "# using the magic ImageNet means and standard deviations. You can try\n",
    "# RandomCrop, RandomHorizontalFlip, etc. during training to obtain\n",
    "# slightly better generalization.\n",
    "\n",
    "# Notice that some augmentation techniques are not normally applied in the validation,\n",
    "# Thus you may need separate data transformation\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=preprocess)\n",
    "\n",
    "# Split the training set into training and validation sets randomly.\n",
    "# CIFAR-10 train contains 50,000 examples, so let's split 80%/20%.\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "# Download the test set. If you use data augmentation transforms for the training set,\n",
    "# you'll want to use a different transformer here.\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=preprocess)\n",
    "\n",
    "# Dataset objects are mainly designed for datasets that can't fit entirely into memory.\n",
    "# Dataset objects don't load examples into memory until their __getitem__() method is\n",
    "# called. For supervised learning datasets, __getitem__() normally returns a 2-tuple\n",
    "# on each call. To make a Dataset object like this useful, we use a DataLoader object\n",
    "# to optionally shuffle then batch the examples in each dataset. During training.\n",
    "# To keep our memory utilization small, we'll use 4 images per batch, but we could use\n",
    "# a much larger batch size on a dedicated GPU. To obtain optimal usage of the GPU, we\n",
    "# would like to load the examples for the next batch while the current batch is being\n",
    "# used for training. DataLoader handles this by spawining \"worker\" threads that proactively\n",
    "# fetch the next batch in the background, enabling parallel training on the GPU and data\n",
    "# loading/transforming/augmenting on the CPU. Here we use num_workers=2 (the default)\n",
    "# so that two batches are always ready or being prepared.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4,\n",
    "                                              shuffle=False, num_workers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
