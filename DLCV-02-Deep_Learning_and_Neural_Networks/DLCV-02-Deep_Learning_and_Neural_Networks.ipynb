{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d08a33",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066061d5",
   "metadata": {},
   "source": [
    "A neural network is inspired by the structure of the human brain. Its basic building block is a neuron, which takes inputs, performs computations, and produces an output. Each neuron has an associated activation function that determines whether it should \"fire\" based on its inputs.  \n",
    "\n",
    "Training a neural network requires to understand a few naming conventions:  \n",
    "#### 1. Architecture and Layers\n",
    "A simple neural network consists of an input layer, a hidden layer and an output layer. For deep neural networks, the hidden layers should be more than one and possibly sometimes the number of input features are also larger.\n",
    "\n",
    "##### Simple Neural Network\n",
    "$x_{1}$, $x_{2}$ and $x_{3}$ are the input features and $y_{1}$ and $y_{2}$ are the outputs. Here, we can say, the model architecture is receiving 3 features and 2 outputs(2 classes) in a classification model.\n",
    "<img src='img/nn.png' alt='Simple Neural Network' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "#### 2. Parameters\n",
    "In between each layers, there are some linear functions which consist of trainable parameters called weights and biases. Basically, the training of a neural network is updating these weights and biases to achieve the objective function of the specific problem.\n",
    "\n",
    "##### Linear function with parameters\n",
    "Let's assume that $x$ is an input feature from a data, the linear function is such a way that the weight $w$ multiple with the input feature $x$ and sum up with the bias $b$. In below graph, we explicitly assign $w=1/2$ and $b=1$ for easy understanding, and it shows a linear line representing that function. This example is only happening in one neuron or one feature. However in neural networks, there will be a lot of similar operations happening in parallel. In such cases, the vectorization and matrix operations are used for faster and more efficient computations.  \n",
    "<img src='img/linear.png' alt='Linear Function' width=\"250px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "\n",
    "#### 3. Activation Function\n",
    "The activation function introduces non-linearity to the network. It decides whether a neuron should be active or not based on its weighted inputs. Common activation functions include sigmoid, tanh, and Rectified Linear Unit (ReLU).\n",
    "\n",
    "<img src='img/activation.png' alt='Activation' width=\"250px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Deep Neural Network\n",
    "Architecture image referenced from: [Industry 4.0 Interoperability, Analytics, Security, and Case Studies](https://www.researchgate.net/figure/a-Typical-Architecture-of-Deep-Learning-Neural-Network-with-One-Output-One-Input-and_fig1_355485828)\n",
    "<img src='img/deepNN.png' alt='Deep Neural Network' width=\"600px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "#### 4. Feedforward\n",
    "In the feedforward neural network, the calculation is only forward pass without updating any parameters yet. The idea is first we calculate the linear function using the input features, then again from the outputs of the linear function, an activation function is introduced in order to get the non-linear outputs. Then the activated outputs from the previous layer go to the next layer as the input of that layer. The process is generally repeated through all the layers until we get the final result $y_{i}$ which is the predicted output. In feedforward, the predicted output may be some way different from the ground truth class.\n",
    "\n",
    "#### 5. Loss Function\n",
    "A loss function measures how far off the neural network's output is from the expected output(label or ground truth). It quantifies the network's performance and guides the learning process. Generally, in deep learning, minimizing the loss between the predicted output and the ground truth is the objective function of the model.\n",
    "\n",
    "#### 6. Optimizer\n",
    "An optimizer is an algorithm or a method used to adjust the parameters (weights and biases) of a neural network during training in order to minimize the loss function(objective function) or maximize the efficiency of the model. The optimizer helps in adjusting the learning rate, momentum and direction of the gradient calculation during training.\n",
    "Examples of optimizer:  \n",
    "\n",
    "1. Stochastic Gradient Descent(SGD)\n",
    "2. Adam\n",
    "3. RMSProp\n",
    "4. Adagrad\n",
    "and so on...  \n",
    "\n",
    "#### 7. Backpropagation\n",
    "Backpropagation is the process of updating the weights and biases in the network to minimize the loss function(objective function). It calculates the gradient of the loss with respect to the network's parameters and updates them using optimization algorithms like Gradient Descent.\n",
    "\n",
    "##### Computational Graph\n",
    "<img src='img/computationgraph1.png' alt='Computation Graph' width=\"250px\" style=\"float: left\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/computationgraph2.png' alt='Computation Graph' width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Derivation\n",
    "<img src='img/backprop1.png' alt='Back Propagation' width=\"300px\" style=\"float: left\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "<img src='img/backprop2.png' alt='Back Propagation' width=\"300px\" style=\"float: left\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "##### Derivative Examples\n",
    "$f(x)=x^{2}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $2x$\n",
    "\n",
    "$f(x)=x^{3}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $3x^{2}$\n",
    "\n",
    "$f(x)=log_{e}(x)$ or $f(x)=ln(x)$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $\\frac{1}{x}$\n",
    "\n",
    "$f(x)=e^{x}$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $e^{x}$\n",
    "\n",
    "$f(x)=2x^{2}+3x+b$ >> $\\frac{\\partial}{\\partial{x}}f(x)$ = $4x+3$ (The derivative of a constant is always zero)\n",
    "\n",
    "#### 8. Training\n",
    "The network is trained using labeled data. During training, the input data is passed through the network, the output is compared to the expected output, and the loss is calculated. Backpropagation then adjusts the weights and biases to minimize the loss so that we will get the optimal weights and biases that fit with our problem.\n",
    "\n",
    "##### Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used in machine learning and deep learning to iteratively update the parameters of a model in order to minimize a loss function. The goal of the algorithm is to find the values of the model's parameters that result in the lowest possible value of the loss function, which in turn signifies the best fit of the model to the training data.  \n",
    "\n",
    "1. Initialization: The algorithm starts with an initial guess for the model's parameters.  \n",
    "\n",
    "2. Compute Loss: The current model's parameters are used to make predictions on the training data. The difference between these predictions and the actual target values (the loss) is calculated using a chosen loss function.  \n",
    "\n",
    "3. Compute Gradient: The gradient of the loss with respect to each parameter is computed. The gradient indicates the direction and magnitude of the steepest increase in the loss function. This step involves taking partial derivatives of the loss function with respect to each parameter.  \n",
    "\n",
    "4. Update Parameters: The parameters are updated by subtracting a fraction of the gradient from the current parameter values. This fraction is called the learning rate. The learning rate determines the step size in the parameter space and influences the speed and stability of convergence.  \n",
    "\n",
    "5. Repeat: Steps 2 to 4 are repeated for a certain number of iterations or until the change in the loss function becomes small (convergence criterion).  \n",
    "\n",
    "Gradient descent algorithm reference: Deep Learning Specialization by Andrew Ng from [Coursera](https://www.coursera.org/).  \n",
    "\n",
    "<img src='img/gradientdescent.png' alt='Gradient Descent Algorithm' width=\"300\" style=\"float: left\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/localoptima.png' alt='Local Optima' width=\"175\" style=\"float: right\" />\n",
    "<!-- <br clear=\"left\" /> -->\n",
    "\n",
    "<img src='img/gradientdescent2.png' alt='Gradient Descent' width=\"400\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "#### 9. Validation and Evaluation\n",
    "After training, the network's performance is evaluated using validation data to ensure it's not overfitting, underfitting, or the model is learning. Once satisfied, the network is tested on unseen data to measure its real-world performance. The testing process is sometimes called inferencing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a26de",
   "metadata": {},
   "source": [
    "# Dealing with Python Packages/ Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a139b7",
   "metadata": {},
   "source": [
    "#### Installing a library with PIP\n",
    "The command to install in your terminal is `pip3 install packagename`. You can find the related package name and it's version in https://pypi.org/.\n",
    "\n",
    "#### Example with Numpy Library\n",
    "Before you start, please make sure that you have already installed `Numpy` library which is a handy package for scientific computing in Python.  \n",
    "To install Numpy, just simply use `pip3 install numpy` or `pip install numpy` in your terminal, or put an exclaimation point infront `!pip3 install numpy` or `!pip install numpy` in you Jupyter cell.  \n",
    "\n",
    "To start using the library, simple import the library in the beginning of your Text editor or Jupyter cell.  \n",
    "`import numpy as np`  \n",
    "Notice that `np` is a shortcut name which you will use that throughout the implementation instead of `numpy`. It is just a standard way of shortcut, however you can name it any shortcut name. It is recommended to use a standard form.  \n",
    "Some other useful libraries you might use in the future:  \n",
    "`import pandas as pd`  \n",
    "`import matplotlib.pyplot as plt`  \n",
    "`from PIL import Image`  \n",
    "`from sklearn.model_selection import train_test_split` << This is importing `train_test_split` method from `model_selection` module from Scikit-Learn(`sklearn`) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd03db",
   "metadata": {},
   "source": [
    "#### Loading and Image Using PIL and Matplotlib Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d24969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # Use for loading image into \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83223779",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = Image.open('img/python.png')\n",
    "print(\"Image object: \", img)\n",
    "img_data = np.array(img)\n",
    "x, y, _ = img_data.shape\n",
    "print(\"Original size: \", (x, y, _))\n",
    "img = img.resize((int(x/4), int(y/4)))\n",
    "print(\"Resized size\", np.array(img).shape)\n",
    "# img.show()\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to read\n",
    "# image using matplotlib\n",
    " \n",
    "# importing matplotlib modules\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Read Images\n",
    "img = mpimg.imread('img/python.png')\n",
    "print(img.shape)\n",
    "height, width, _ = img.shape\n",
    "# Output Images\n",
    "# plt.figure(figsize=(height/72, width/72))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd3c59",
   "metadata": {},
   "source": [
    "#### Loading an Image Using OpenCV\n",
    "\n",
    "\n",
    "To load the image, we will try a different method by using a library called OpenCV. First, we will need the installation of `opencv-python`.  \n",
    "\n",
    "`pip install opencv-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e205a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    " \n",
    "# Load the image\n",
    "image = cv2.imread(\"img/cat.png\")\n",
    " \n",
    "# # Display the image\n",
    "# cv2.imshow(\"Image\", image)\n",
    " \n",
    "# # Wait for the user to press a key\n",
    "# cv2.waitKey(0)\n",
    " \n",
    "# # Close all windows\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c0385",
   "metadata": {},
   "source": [
    "# Vectorization and Matrix Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f56efd",
   "metadata": {},
   "source": [
    "#### Different types of vector/matrix operations\n",
    "1. The dot product (inner scalar product)  \n",
    "$\\mathbf{x1} \\cdot \\mathbf{x2}$  \n",
    "<img src=\"img/dot.png\" alt=\"Dot Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "2. Elementwise product (Hadamard product)  \n",
    "$\\mathbf{x1} \\circ \\mathbf{x2}$  \n",
    "$\\mathbf{x1} \\odot \\mathbf{x2}$  \n",
    "<img src=\"img/hadamard.png\" alt=\"Hadamard Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  \n",
    "\n",
    "3. The outer product  \n",
    "$\\mathbf{x1} \\otimes \\mathbf{x2}$  \n",
    "<img src=\"img/outer.png\" alt=\"Outer Product\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658d7d9",
   "metadata": {},
   "source": [
    "#### Comparing the running times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing libraries before writing your codes\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "# np.random.seed(27)\n",
    "# x1 = np.random.randint(500, 1000, 500)\n",
    "# x2 = np.random.randint(0, 500, 500)\n",
    "# print(\"x1[0]:\", x1[0], \"  \", \"x2[0]:\", x2[0])\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"dot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"outer ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"elementwise multiplication ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"gdot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "# np.random.seed(27)\n",
    "# x1 = np.random.randint(500, 1000, 500)\n",
    "# x2 = np.random.randint(0, 500, 500)\n",
    "# print(\"x1[0]:\", x1[0], \"  \", \"x2[0]:\", x2[0])\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"dot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"outer ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"elementwise multiplication ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "# print (\"gdot ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6629eeb",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534649c",
   "metadata": {},
   "source": [
    "1. Create three matrices and print out the results:  \n",
    "$\\mathbf{A}$ = $\\begin{pmatrix} 2&1 \\\\ 6&5  \\end{pmatrix}$, \n",
    "$\\mathbf{B}$ = $\\begin{pmatrix} 3&4 \\\\ 8&7  \\end{pmatrix}$,\n",
    "$\\mathbf{C}$ = $\\begin{pmatrix} 3&4&0 \\\\ 8&7&9  \\end{pmatrix}$  \n",
    "\n",
    "2. Calculate the following and print out the results:  \n",
    "\n",
    "    1. $\\mathbf{A} \\cdot \\mathbf{B}$ >> Try 3 different styles using `@` sign or `np.dot()` or `np.matmul()` functions. All three results should be the same.  \n",
    "    2. $\\mathbf{A} \\cdot \\mathbf{C}$\n",
    "    3. $2\\mathbf{A}$\n",
    "    4. $2\\mathbf{A}^{T}$ >> To transpose the matrix, you can simply use `matrix.T` or `np.transpose()` function.\n",
    "    5. $3\\mathbf{A}^{T} \\cdot \\mathbf{B}$ >> Print out the result and shape.\n",
    "    6. $3\\mathbf{A}^{T} \\cdot \\mathbf{C}$ >> Print out the result and shape.\n",
    "    7. $\\mathbf{A} \\odot \\mathbf{B}$\n",
    "    8. $\\mathbf{A} \\odot \\mathbf{C}$ >> This should show up error.  \n",
    "3. Comment the difference between $\\mathbf{A} \\cdot \\mathbf{B}$ and  $\\mathbf{A} \\odot \\mathbf{B}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "# 1. Create matrices\n",
    "\n",
    "# A . B (Three different styles)\n",
    "\n",
    "# A . C\n",
    "\n",
    "# 2A\n",
    "\n",
    "# 2A.T\n",
    "\n",
    "# 3A.T . B (Print out shape and result)\n",
    "\n",
    "# 3A.T . C (Print out shape and result)\n",
    "\n",
    "# A * B\n",
    "\n",
    "# A * C (error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485b19b",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "A . B = [[14 15]\n",
    " [58 59]]\n",
    "\n",
    "A . C = [[14 15  9]\n",
    " [58 59 45]]\n",
    "\n",
    "2A = [[ 4  2]\n",
    " [12 10]]\n",
    "\n",
    "2A.T = [[ 4 12]\n",
    " [ 2 10]]\n",
    "\n",
    "3A.T . B = [[162 150]\n",
    " [129 117]]\n",
    "\n",
    "Shape of 3A.T . B = (2, 2)\n",
    "\n",
    "3A.T . C = [[162 150 162]\n",
    " [129 117 135]]\n",
    " \n",
    "Shape of 3A.T . C = (2, 3)  \n",
    "A * B = [[ 6  4]\n",
    " [48 35]]\n",
    " \n",
    "A * C = Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc7bf7",
   "metadata": {},
   "source": [
    "#### Reshaping arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3174cc8",
   "metadata": {},
   "source": [
    "Two common numpy functions used in deep learning are np.shape and np.reshape().  \n",
    "- `X.shape` is used to get the shape (dimension) of a matrix/vector $\\mathbf{X}$\n",
    "- `X.reshape(...)` is used to reshape $\\mathbf{X}$ into some other dimension.  \n",
    "\n",
    "In computer science, an image is represented by a 3D array of shape (height, width, channel). However, when you read an image as the input of an algorithm you convert it to a vector of shape (height * width * channel, 1).  \n",
    "\n",
    "<img src=\"img/image2vector.png\" alt=\"Image to Vector\" width=\"600px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bded1d",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54000e",
   "metadata": {},
   "source": [
    "Implement `image2vector()` function that takes an input of shape (height, width, 3) and returns a vector of shape (height * width * 3, 1).  \n",
    "\n",
    "*Do not hardcode the dimensions of image as a constant. The dimensions can varies from different images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Convert image with 3 dimensions to become vector of (size, 1)\n",
    "    \"\"\"\n",
    "    v = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))\n",
    "\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"img/lena.png\")\n",
    "\n",
    "print(img.shape)\n",
    "vecimg = image2vector(img)\n",
    "print(vecimg.shape)\n",
    "\n",
    "assert image2vector(image).shape == (image.shape[0] * image.shape[1] * image.shape[2], 1), \"Dimension is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144b2df",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "image2vector(image) = [[0.67826139] \\\n",
    " [0.29380381] \\\n",
    " [0.90714982] \\\n",
    " [0.52835647] \\\n",
    " [0.4215251 ]\\\n",
    " [0.45017551]\\\n",
    " [0.92814219]\\\n",
    " [0.96677647]\\\n",
    " [0.85304703]\\\n",
    " [0.52351845]\\\n",
    " [0.19981397]\\\n",
    " [0.27417313]\\\n",
    " [0.60659855]\\\n",
    " [0.00533165]\\\n",
    " [0.10820313]\\\n",
    " [0.49978937]\\\n",
    " [0.34144279]\\\n",
    " [0.94630077]]\\\n",
    "(512, 512, 3)\\\n",
    "(786432, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c5341",
   "metadata": {},
   "source": [
    "#### Matrix Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c7c03",
   "metadata": {},
   "source": [
    "Normalization is a technique for machine learning and deep learning. It generally ensures a better performance sicne the gradient descent converges faster after the data is normalized. The normalization also controls the parameters not to overflow while updating the gradients. For example, we have a matrix $\\mathbf{A}$  \n",
    "\n",
    "$$\\mathbf{A} = \\left[ \\begin{array}{ccc}\n",
    "                      8 & 1 & 2 \\\\\n",
    "                      3 & 9 & 5 \\\\\n",
    "                      \\end{array}\\right]$$\n",
    "\n",
    "We can get the norm the matrix using `np.linalg.norm()` function. In our case, we find the norm along the rows. By using numpy library function, we can write the code as follows:  \n",
    "\n",
    "$$ \\|A\\| = np.linalg.norm(\\mathbf{A}, axis = 1) $$\n",
    "\n",
    "and normalize the matrix by simply dividing the original matrix by the norm.\n",
    "\n",
    "$$ norm(\\mathbf{A}) = \\frac{\\mathbf{A}}{\\|\\mathbf{A}\\|} $$\n",
    "\n",
    "##### Broadcasting\n",
    "\n",
    "Notice that numpy has a feature called broadcasting which allows the operation of matrices with different sizes by duplicating to get the same size. (In our case, it duplicates the norm matrix into two rows before doing the division)  \n",
    "\n",
    "$$ \\left[ \\begin{array}{ccc}\n",
    "                      8 & 1 & 2 \\\\\n",
    "                      3 & 9 & 5 \\\\\n",
    "                      \\end{array}\\right] / \n",
    "\\left[ \\begin{array}{c}\n",
    "                      norm_{0} \\\\\n",
    "                      norm_{1} \\\\\n",
    "                      \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{ccc}\n",
    "                      8 & 1 & 2 \\\\\n",
    "                      3 & 9 & 5 \\\\\n",
    "                      \\end{array}\\right] / \n",
    "\\left[ \\begin{array}{ccc}\n",
    "                      norm_{0} & norm_{0} & norm_{0} \\\\\n",
    "                      norm_{1} & norm_{1} & norm_{1}\\\\\n",
    "                      \\end{array}\\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba12f13",
   "metadata": {},
   "source": [
    "\n",
    "Implement `normalizeRows()` to normalize the rows of a matrix. After applying this function to an input matrix $\\mathbf{x}$, each row of $\\mathbf{x}$ should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71374f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf26c36",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1902504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_x = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    return norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "A = np.array([\n",
    "    [8, 1, 2],\n",
    "    [3, 9, 5]])\n",
    "\n",
    "norm_A = normalizeRows(A)\n",
    "print(\"normalizeRows(A) = \" + str(norm_A))\n",
    "\n",
    "sqr_A = norm_A * norm_A\n",
    "sum_A = np.sum(sqr_A, axis = 1)\n",
    "print(\"prove of A in each row\", sum_A)\n",
    "\n",
    "assert sum_A.shape == (2,), \"normalize must do in row\" \n",
    "assert np.round(sum_A[0], 1) == 1 and np.round(sum_A[1], 1) == 1, \"Normalize incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55089c3",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "normalizeRows(A) = [[0.96308682 0.12038585 0.24077171]\\\n",
    " [0.27975144 0.83925433 0.4662524 ]]\\\n",
    "prove of A in each row [1. 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3162919",
   "metadata": {},
   "source": [
    "# Implement the L1 and L2 loss functions\n",
    "\n",
    "The loss L1 and L2 are used to evaluate the performance of your model. The bigger your loss is, the more different your predictions $\\hat{h}$ are from the true values $y$. In deep learning, Gradient Descent or Ascent is used to optimize models by minimizing the cost.\n",
    "\n",
    "To assume loss function in $L_1$, the L1 loss is defined as\n",
    "\n",
    "$$L_1(\\hat{y},y)=\\sum_{i=0}^m |y^{(i)}-\\hat{y}^{(i)}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296f3d7",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Implement the numpy vectorized version of the L1 loss. use function np.abs() to apply the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75449c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dae744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "yhat2 = np.array([.1, 0.7, 0.4, 0.7, .8, 0.2])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "y2 = np.array([1, 0, 0, 1, 1, 0])\n",
    "l1 = L1(yhat,y)\n",
    "l2 = L1(yhat2,y2)\n",
    "print(\"L1 of output 1 = \" + str(l1))\n",
    "print(\"L1 of output 2 = \" + str(l2))\n",
    "\n",
    "assert np.round(l1,1) == 1.1, \"L1 loss is incorrect\" \n",
    "assert np.round(l2,1) == 2.7, \"L1 loss is incorrect\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3052d",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "L1 of output 1 = 1.1 \\\n",
    "L1 of output 2 = 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a4764",
   "metadata": {},
   "source": [
    "To assume loss function in $L_2$, the L2 loss is defined as\n",
    "\n",
    "$$L_2(\\hat{y},y)=\\sum_{i=0}^m (y^{(i)}-\\hat{y}^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdb7e7",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Implement the numpy vectorized version of the L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def L2(yhat, y):\n",
    "    loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "yhat2 = np.array([.1, 0.7, 0.4, 0.7, .8, 0.2])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "y2 = np.array([1, 0, 0, 1, 1, 0])\n",
    "l1 = L2(yhat,y)\n",
    "l2 = L2(yhat2,y2)\n",
    "print(\"L2 of output 1 = \" + str(l1))\n",
    "print(\"L2 of output 2 = \" + str(l2))\n",
    "\n",
    "assert np.round(l1,2) == 0.43, \"L2 loss is incorrect\" \n",
    "assert np.round(l2,2) == 1.63, \"L2 loss is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7655fc",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "L2 of output 1 = 0.43 \\\n",
    "L2 of output 2 = 1.6300000000000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fe832",
   "metadata": {},
   "source": [
    "# Break Down of a Neural Network Classification Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96306252",
   "metadata": {},
   "source": [
    "A neural network training can be broken down into the following subtasks:  \n",
    "\n",
    "1. Data Preprocessing\n",
    "2. Forward Propagation\n",
    "3. Loss Function\n",
    "4. Back Propagation\n",
    "5. Parameters Update\n",
    "6. Hyperparameter Tuning\n",
    "\n",
    "We will implement these subtasks step by step from scratch in order to have a better understanding of the background process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9a422",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "This step includes:  \n",
    "a. loading the input images and visualizing them  \n",
    "b. Getting one hot encoded labels  \n",
    "c. Normalization  \n",
    "d. Splitting data to training, validation and test sets  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18979ba",
   "metadata": {},
   "source": [
    "#### Loading Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c58ba5",
   "metadata": {},
   "source": [
    "The dataset we are going to use in this training is the images of handwritten digits which we can get from `sklearn` library. Firstly, we have to import the required libraries as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d19366",
   "metadata": {},
   "source": [
    "Then, load the dataset. and see the data and shape of X(inputs) and y(labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_digits()\n",
    "\n",
    "y_indices = data.target # load the targets(labels)\n",
    "X = np.matrix(data.data) # load the input images\n",
    "\n",
    "print('y shape:', y_indices.shape)\n",
    "print('X shape:', X.shape)\n",
    "\n",
    "print('y data:', y_indices[0:15])\n",
    "print('X data:', X[0:5])\n",
    "\n",
    "data_size = X.shape[0]\n",
    "x_area = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7d32d",
   "metadata": {},
   "source": [
    "The shape of X is (1797, 64) which means we have 1797 samples of data in (1, 64) long vector. Since each sample represents 1 D vector and the actual size of an image is 8x8, we need to convert the size of input X into 8x8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1da55",
   "metadata": {},
   "source": [
    "#### Visualize Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd9b11",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbe29f",
   "metadata": {},
   "source": [
    "Write a function to convert the X data of one image to be image size 8x8. Check the size of input vector to convert image and verify after conversion.  \n",
    "\n",
    "**Hint**: use np.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def convert_image(X_one_image):\n",
    "    img = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4059614",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "img_0 = convert_image(X[0])\n",
    "plt.imshow(img_0, 'gray')\n",
    "plt.title('Example MNIST sample (category %d)' % y_indices[0])\n",
    "plt.show()\n",
    "\n",
    "img_5 = convert_image(X[5,:])\n",
    "plt.imshow(img_5, 'gray')\n",
    "plt.title('Example MNIST sample (category %d)' % y_indices[5])\n",
    "plt.show()\n",
    "\n",
    "test_v = np.empty([1,256])\n",
    "test = convert_image(test_v)\n",
    "\n",
    "assert img_0.shape == (8,8) and img_5.shape == (8,8), 'Image reshape is incorrect'\n",
    "assert test.shape == (16,16), 'Image reshape is incorrect'\n",
    "assert img_0[3,6] == X[0, 30] and img_5[4,2] == X[5, 34], 'Image reshape is incorrect'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2949b10",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<img src=\"img/digit0.png\" title=\"Expect value 0\" style=\"width: 200px;\" />\n",
    "<img src=\"img/digit5.png\" title=\"Expect value 5\" style=\"width: 200px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162cf993",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d339a5f",
   "metadata": {},
   "source": [
    "As you can see, the output y is index value. To use the value for classify in deep learning, you need to convert it to one hot matrix.  \n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.  \n",
    "\n",
    "To do so, we need to convert the index value to the following format:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975fe08",
   "metadata": {},
   "source": [
    "$$0 \\rightarrow \\left[1,0,0,0,0,0,0,0,0,0\\right] $$\n",
    "$$1 \\rightarrow \\left[0,1,0,0,0,0,0,0,0,0\\right] $$\n",
    "$$2 \\rightarrow \\left[0,0,1,0,0,0,0,0,0,0\\right] $$\n",
    "$$3 \\rightarrow \\left[0,0,0,1,0,0,0,0,0,0\\right] $$\n",
    "$$4 \\rightarrow \\left[0,0,0,0,1,0,0,0,0,0\\right] $$\n",
    "$$5 \\rightarrow \\left[0,0,0,0,0,1,0,0,0,0\\right] $$\n",
    "$$6 \\rightarrow \\left[0,0,0,0,0,0,1,0,0,0\\right] $$\n",
    "$$7 \\rightarrow \\left[0,0,0,0,0,0,0,1,0,0\\right] $$\n",
    "$$8 \\rightarrow \\left[0,0,0,0,0,0,0,0,1,0\\right] $$\n",
    "$$9 \\rightarrow \\left[0,0,0,0,0,0,0,0,0,1\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663dbb",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748755e",
   "metadata": {},
   "source": [
    "Implement one-hot vector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3832813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def convert_to_one_hot(y, onehot_size):\n",
    "    y_vect = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "y = convert_to_one_hot(y_indices, 10)\n",
    "print(y.shape)\n",
    "print(y[3])\n",
    "assert y.shape[1] == 10 and y.shape[0] == 1797, \"One hot size is incorrect\"\n",
    "assert y[14, 8] == 0 and y[177,1] == 1, \"One hot value is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc07645",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "(1797, 10)\\\n",
    "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ffb47",
   "metadata": {},
   "source": [
    "#### Normalize Input Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c7e0d",
   "metadata": {},
   "source": [
    "Now, change the input $\\mathbf{x}$ to be a normalized input vector. The normalization equation is:\n",
    "\n",
    "$$ norm(\\mathbf{x}) = \\frac{\\mathbf{x}-\\mathbf{\\tilde{x}}}{\\sigma} $$\n",
    "\n",
    "where,  \n",
    "$\\mathbf{\\tilde{x}}$ = Mean  \n",
    "$\\sigma$ = Standard Deviation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c02a7b",
   "metadata": {},
   "source": [
    "#### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08d775",
   "metadata": {},
   "source": [
    "Write a normalization code. If some values are nan, please change them to be zero, using np.nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def normalize(X):\n",
    "    XX = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "print(XX[0])\n",
    "\n",
    "assert XX.shape == X.shape, \"Normalize function is incorrect\"\n",
    "assert np.max(XX[0]) < 2 and np.min(XX[0]) > -2, \"Data is not normalize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56422641",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "[[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751 -0.84412939\\\n",
    "  -0.40972392 -0.12502292 -0.05907756 -0.62400926  0.4829745   0.75962245\\\n",
    "  -0.05842586  1.12772113  0.87958306 -0.13043338 -0.04462507  0.11144272\\\n",
    "   0.89588044 -0.86066632 -1.14964846  0.51547187  1.90596347 -0.11422184\\\n",
    "  -0.03337973  0.48648928  0.46988512 -1.49990136 -1.61406277  0.07639777\\\n",
    "   1.54181413 -0.04723238  0.          0.76465553  0.05263019 -1.44763006\\\n",
    "  -1.73666443  0.04361588  1.43955804  0.         -0.06134367  0.8105536\\\n",
    "   0.63011714 -1.12245711 -1.06623158  0.66096475  0.81845076 -0.08874162\\\n",
    "  -0.03543326  0.74211893  1.15065212 -0.86867056  0.11012973  0.53761116\\\n",
    "  -0.75743581 -0.20978513 -0.02359646 -0.29908135  0.08671869  0.20829258\\\n",
    "  -0.36677122 -1.14664746 -0.5056698  -0.19600752]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e27372",
   "metadata": {},
   "source": [
    "#### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6b4ac",
   "metadata": {},
   "source": [
    "In a machine learning training, it is necessary to split the existing data into three different types:\n",
    "\n",
    "1. Training set - Data that is used for training the model\n",
    "2. Validation set - Data that is used for validating the model performance throughout the training at each epoch or iteration. The validation data is used to verify the model is learning and not to overfit or underfit the training data.\n",
    "3. Testing set - Data that is used in a process called inferencing. The test set is totally unseen by the model during training and validation process. Higher performance of the model on the testing data means the model is robust and generalizable to unseen data.\n",
    "\n",
    "The process of spliting the training, validate, and test set needs the following criteria:\n",
    "\n",
    "1. The data need to be random before split.\n",
    "2. The validate and test set can be from the same distribution as training data, however the same data must be strictly in only one split set(train or validation or test).\n",
    "3. More data is almost always helps, however the consistency and reliability of data are key importances when selection of data(garbage in garbage out).  \n",
    "\n",
    "Normally, we should split data in percentage. However, this is not fixed. You can adjust.  \n",
    "\n",
    "- 60% training, 20% validate, and 20% test\n",
    "- 80% training, 10% validate, and 10% test\n",
    "- For the very low data (~1000 data), we could use validate and test set in the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb24b7",
   "metadata": {},
   "source": [
    "#### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87949180",
   "metadata": {},
   "source": [
    "Random split the train set to be 60% of data and the rest is the test set.\n",
    "\n",
    "Hint: use `np.arange()` for set index number from 0 to data_size. Random index can do by using `random.shuffle()`. Please make sure that you are using normalized data to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "import random\n",
    "\n",
    "percent_train = .6\n",
    "# arange index number from 0 to data_size\n",
    "idx = None\n",
    "# random shuffle idx (1 line)\n",
    "\n",
    "# calculate number of training set\n",
    "m_train = None\n",
    "# split train_idx and test_idx (uncomment these 2 lines)\n",
    "# train_idx = idx[0:m_train]\n",
    "# test_idx = idx[m_train:data_size+1]\n",
    "\n",
    "# split to X_train and X_test\n",
    "X_train = None\n",
    "X_test = None\n",
    "\n",
    "# split to y_train y_test and y_test_indices\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_test_indices = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "assert X_train.shape[0] == int(percent_train * data_size), \"training size is incorrect\"\n",
    "assert data_size - m_train == X_test.shape[0], \"test size is incorrect\"\n",
    "assert train_idx[0] != 0 and train_idx[25] != 25, \"training indices are not shuffled\"\n",
    "assert X_train.shape == (m_train, XX.shape[1]) and y_train.shape[0] == m_train\n",
    "assert X_test.shape == (data_size - m_train, XX.shape[1]) and y_test.shape[0] == data_size - m_train and y_test_indices.shape[0] == data_size - m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28576e",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849c920",
   "metadata": {},
   "source": [
    "The neural network we are going to implement looks very similar to the below diagram. However, our input size is 8x8 flattened into (1, 64) vector.\n",
    "\n",
    "<img src=\"img/digits_model.png\" alt=\"Neural Network for Digits Classification\" width=\"400px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "A few steps we will implement in forward propagation or feed forward network. \n",
    "\n",
    "a. Building activation functions  \n",
    "b. Initializing parameters  \n",
    "c. Feed forward layer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23677d2",
   "metadata": {},
   "source": [
    "### Building Activation Functions with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efbaa1",
   "metadata": {},
   "source": [
    "Referenced from [Activation functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990ae47",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "Sigmoid function is known as the logistic function. The equation can be written as: \n",
    "\n",
    "$$\\textit{sigmoid(x)}=\\frac{1}{1+e^{-\\textit{x}}}$$\n",
    "\n",
    "<img src='img/sigmoid.png' alt='Sigmoid Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83969681",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4470636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def math_sigmoid(x):\n",
    "    '''\n",
    "    Compute sigmoid of x\n",
    "    '''\n",
    "    z = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "print(math_sigmoid(5))\n",
    "\n",
    "assert math_sigmoid(10) > 0.9999, \"Calculate error\"\n",
    "assert math_sigmoid(-10) < 0.0001, \"Calculate error\"\n",
    "assert math_sigmoid(0) == 0.5, \"Calculate error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37c23",
   "metadata": {},
   "source": [
    "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18098dd4",
   "metadata": {},
   "source": [
    "In fact, if $\\mathit{x = (x_{1}, x_{2},...,x_{n})}$ is a row vector then will apply the exponential function to every element of $\\mathit{x}$. The output will thus be:  \n",
    "\n",
    "$$\\mathit{np.exp(x) = (e^{x_{1}},e^{x_{2}},...,e^{x_{n}})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d98a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61b854",
   "metadata": {},
   "source": [
    "#### Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "# Use Numpy library to build your sigmoid function again\n",
    "\n",
    "def Sigmoid(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Sigmoid(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7109, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.5498, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.5250, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.4256, \"sigmoid output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.3318, \"sigmoid output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70f95",
   "metadata": {},
   "source": [
    "**Expected Output**: [0.7109495  0.549834   0.52497919 0.42555748 0.33181223]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb00587",
   "metadata": {},
   "source": [
    "#### ReLU (Rectified Linear Unit)\n",
    "A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: $\\textit{max(0,z)}$\n",
    ". Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.  \n",
    "\n",
    "$$\\textit{ReLU(x)} = \\textit{max(0, x)}$$  \n",
    "\n",
    "<img src='img/relu.png' alt='ReLU Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd4f15",
   "metadata": {},
   "source": [
    "#### Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def ReLu(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45246787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = ReLu(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"ReLu output is incorrect\"\n",
    "assert y_hat[3] > a[3] and y_hat[3] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[4] > a[4] and y_hat[4] == 0, \"ReLu output is incorrect\"\n",
    "assert y_hat[0] == a[0], \"ReLu output is incorrect\"\n",
    "assert y_hat[1] == a[1], \"ReLu output is incorrect\"\n",
    "assert y_hat[2] == a[2], \"ReLu output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdedb0e",
   "metadata": {},
   "source": [
    "**Expected Output**: [0.9 0.2 0.1 0.  0. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07138b1",
   "metadata": {},
   "source": [
    "#### Tanh (Hyperbolic Tangent)\n",
    "Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.  \n",
    "\n",
    "$$\\textit{Tanh(x)} = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$  \n",
    "\n",
    "<img src='img/tanh.png' alt='Tanh Function' width=\"200px\" style=\"float: center\" />\n",
    "<br clear=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f62555",
   "metadata": {},
   "source": [
    "#### Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Tanh(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Tanh(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.7163, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.1974, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.0997, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == -0.2913, \"Tanh output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == -0.6044, \"Tanh output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc11a35",
   "metadata": {},
   "source": [
    "**Expected Output**: [ 0.71629787  0.19737532  0.09966799 -0.29131261 -0.60436778]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bfa05",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "Softmax function calculates the probabilities distribution of the event over ‘i’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.  \n",
    "\n",
    "$$Softmax(z_{i}) = \\frac{exp(z_{i})}{\\sum_{j=1}^{K}{exp(z_{j})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ed1bb",
   "metadata": {},
   "source": [
    "#### Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax for each row of the input x.\n",
    "\n",
    "    The code should work for a row vector and also for matrices of shape (m,n).\n",
    "    \"\"\"\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccf10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Softmax(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.4083, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.2028, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.1835, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.1230, \"Softmax output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.0824, \"Softmax output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd2470",
   "metadata": {},
   "source": [
    "**Expected Output**: [0.4083291  0.20277023 0.18347409 0.12298636 0.08244022]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1f873",
   "metadata": {},
   "source": [
    "### Initializing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b77f7",
   "metadata": {},
   "source": [
    "Each layer contains weight vector $\\mathit{w}$ and bias $\\mathit{b}$. We will create a set of random values with a normal distribution with mean zero and standard distribution 0.1. We create 3 layers as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = 5\n",
    "h1 = 6\n",
    "\n",
    "# Initialize the weights at each node at each layer. We do not have weights at the input layer thus it is an empty list.\n",
    "W = [[], np.random.normal(0,0.1,[x_area,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "# Initialize the bias at each node at each layer. We do not have bias at the input layer thus it is an empty list.\n",
    "B = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "\n",
    "# putting all the non-linear activation functions into a list.\n",
    "# Layer 0 input layer has no activation\n",
    "# Layer 1 activation is ReLu\n",
    "# Layer 2 activation is Sigmoid\n",
    "# The output layer activation is Softmax\n",
    "act_funcs = [None, ReLu, Sigmoid, Softmax]\n",
    "\n",
    "L = len(W)-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f05c9",
   "metadata": {},
   "source": [
    "### Feed Forward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf100db1",
   "metadata": {},
   "source": [
    "For input $\\mathit{x^{(i)}}$ at i-th layer, the forward propagation can be calculated by:  \n",
    "\n",
    "$$\\mathit{z^{(i)}} = \\mathbf{W}^{T}\\mathit{x^{(i)}}+\\mathbf{b}$$  \n",
    "\n",
    "$$\\mathit{\\hat{y}^{(i)}} = \\mathit{a^{(i)}} = act{(\\mathit{z^{(i)}})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1383d98",
   "metadata": {},
   "source": [
    "#### Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de346f7",
   "metadata": {},
   "source": [
    "Create forward_layer function which input self-define activation function  \n",
    "\n",
    "Note: If input act_func as `None`, the output is linear activation function  \n",
    "\n",
    "Hint: use `*` for multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def forward_layer(w, b, X, act_func):\n",
    "    # z is linear function\n",
    "    z = None\n",
    "    # y_hat is output after activation function\n",
    "    y_hat = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ab614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test function - do not remove\n",
    "\n",
    "X = np.array([[.9, 0.2, 0.1, -0.3, -0.7]]).T\n",
    "\n",
    "w = np.array([[0.2, 0.1, 1, 3, 0.5]])\n",
    "b = np.array([[1]])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, b, X, None)\n",
    "b = np.array([[0.5]])\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, None)\n",
    "print('Linear output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1[2,0] == 1.1\n",
    "assert np.round(y_hat2[3,0], 4) == -0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d606c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, ReLu)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, ReLu)\n",
    "print('ReLu output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1[3] > 0, \"Forward layer is incorrect\"\n",
    "assert y_hat2[3] == 0, \"Forward layer is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, Tanh)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, Tanh)\n",
    "print('Tanh output of y_hat1', y_hat1, 'and', y_hat2)\n",
    "\n",
    "assert y_hat1.shape[0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "w = np.array([0.2, 0.1, 1, 3, 0.5])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, 1, X, Sigmoid)\n",
    "z2, y_hat2 = forward_layer(w, 0.5, X, Sigmoid)\n",
    "print('Sigmoid output of y_hat1', y_hat1, 'and', y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "X = np.array([[.9, 0.2, 0.1, -0.3, -0.7]]).T\n",
    "\n",
    "w = np.array([[0.2, 0.1, 1, 3, 0.5], [0.3, 0.5, 0.1, -0.3, -0.5]])\n",
    "b = np.array([-1,3])\n",
    "\n",
    "z1, y_hat1 = forward_layer(w, b, X, Softmax)\n",
    "print('Linear output of z1', z1)\n",
    "print('Softmax output of y_hat1', y_hat1)\n",
    "\n",
    "assert y_hat1.shape == (5, 2), \"Forward layer is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f2718",
   "metadata": {},
   "source": [
    "**Expected Output**:\\\n",
    "Linear output of z1 [[-0.82  3.27]\\\n",
    " [-0.98  3.1 ]\\\n",
    " [-0.9   3.01]\\\n",
    " [-1.9   3.09]\\\n",
    " [-1.35  3.35]]\\\n",
    "Softmax output of y_hat1 [[0.00364271 0.21761522]\\\n",
    " [0.00310411 0.18359431]\\\n",
    " [0.00336265 0.16779256]\\\n",
    " [0.00123705 0.18176751]\\\n",
    " [0.00214412 0.23573976]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59733e",
   "metadata": {},
   "source": [
    "#### Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f55fea",
   "metadata": {},
   "source": [
    "Create full of forward propagation\n",
    "\n",
    "Calcualate only z_layer and a_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e17106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def forward_one_step(X, W, B, act_funcs):\n",
    "    L = len(W)-1\n",
    "    a = [X]\n",
    "    z = [[]]\n",
    "    delta = [[]]\n",
    "    dW = [[]]\n",
    "    db = [[]]\n",
    "    for l in range(1,L+1):\n",
    "        z_layer, a_layer = None, None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        z.append(z_layer)\n",
    "        a.append(a_layer)\n",
    "        # Just to give arrays the right shape for the backprop step\n",
    "        delta.append([]); dW.append([]); db.append([])\n",
    "    return a, z, delta, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ae573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "\n",
    "a, z, delta, dW, db = forward_one_step(x_this, W, B, act_funcs)\n",
    "print('size of a', len(a), 'a[3] =', a[3])\n",
    "print('size of z', len(z), 'z[3] =', z[3])\n",
    "\n",
    "assert len(a) == len(z) and len(a) == 4\n",
    "assert a[0].shape == (64,1)\n",
    "assert a[1].shape == (6,1)\n",
    "assert a[2].shape == (5,1)\n",
    "assert a[3].shape == (10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a39e5f",
   "metadata": {},
   "source": [
    "**Expected Output** (The output may not the same):\\\n",
    "size of a 4 a[3] = [[0.09043188]\\\n",
    " [0.11294963]\\\n",
    " [0.06708026]\\\n",
    " [0.11812721]\\\n",
    " [0.11817167]\\\n",
    " [0.10483097]\\\n",
    " [0.09818055]\\\n",
    " [0.08948474]\\\n",
    " [0.10362842]\\\n",
    " [0.09711466]]\\\n",
    "size of z 4 z[3] = [[-0.05075633]\\\n",
    " [ 0.17158877]\\\n",
    " [-0.34946347]\\\n",
    " [ 0.21640886]\\\n",
    " [ 0.2167852 ]\\\n",
    " [ 0.096996  ]\\\n",
    " [ 0.03145495]\\\n",
    " [-0.06128506]\\\n",
    " [ 0.0854584 ]\\\n",
    " [ 0.02053917]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd88ed4",
   "metadata": {},
   "source": [
    "## 3. Loss function\n",
    "\n",
    "For softmax loss function, it is cross entropy loss. You can calculate as\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i=0}^n (y_i * \\log\\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d34f5",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "\n",
    "Create loss function for multi classification (cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def loss(y, yhat):\n",
    "    l = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "y_hat = np.array([0.4083291, 0.20277023, 0.18347409, 0.12298636, 0.08244022])\n",
    "y = np.array([0, 1, 0, 0, 0])\n",
    "\n",
    "l = loss(y, y_hat)\n",
    "print(l)\n",
    "\n",
    "assert np.round(l, 4) == 1.5957, \"Loss function incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d807072",
   "metadata": {},
   "source": [
    "**Expected Output**: 1.5956818129123256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17a275",
   "metadata": {},
   "source": [
    "## 4. Back propagation\n",
    "\n",
    "Back propagation can be calculated as\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial z^{[l-1]} } =[W^{[l]}]^T \\cdot \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} } * {g^{[l-1]}}'(z^{[l-1]})$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial W^{[l]} } = \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} } \\cdot [a^{[l-1]}]^T$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial b^{[l]} } = \\frac{ \\partial\\mathcal{L} }{\\partial z^{[l]} }$$\n",
    "\n",
    "When ${g^{[l-1]}}'$ is derivative activation function.\n",
    "\n",
    "Thus first of all, we need to calculate derivative of the activation functions that we use.\n",
    "\n",
    "The Linear_derivative ($dl$) function is\n",
    "$$dl(x) = [1]$$\n",
    "\n",
    "The ReLu_derivative ($dReLu$) function is\n",
    "\n",
    "$$dReLu(x) = [\\text{1 when x>0, otherwise 0}]$$\n",
    "\n",
    "The Tanh_derivative ($dTanh$) function is\n",
    "\n",
    "$$dTanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "The Sigmoid_derivative ($ds$) function is\n",
    "\n",
    "$$ds(x) = sigmoid(x)(1-sigmoid(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15d340",
   "metadata": {},
   "source": [
    "#### Exercise 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530bb82",
   "metadata": {},
   "source": [
    "Write the derivative functions as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Linear_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Linear_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[0] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[1] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[2] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[3] == 1, \"Linear_derivative output is incorrect\"\n",
    "assert y_hat[4] == 1, \"Linear_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e662198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def ReLu_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2133959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = ReLu_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[0] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[1] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[2] == 1, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[3] == 0, \"ReLu_derivative output is incorrect\"\n",
    "assert y_hat[4] == 0, \"ReLu_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb657fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Tanh_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6adb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Tanh_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.4869, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.9610, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.9901, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.9151, \"Tanh_derivative output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.6347, \"Tanh_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def Sigmoid_derivative(x):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "a = np.array([.9, 0.2, 0.1, -0.3, -0.7])\n",
    "\n",
    "y_hat = Sigmoid_derivative(a)\n",
    "print(y_hat)\n",
    "\n",
    "assert y_hat.shape[0] == 5, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[0],4) == 0.2055, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[1],4) == 0.2475, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[2],4) == 0.2494, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[3],4) == 0.2445, \"Sigmoid_derivative output is incorrect\"\n",
    "assert np.round(y_hat[4],4) == 0.2217, \"Sigmoid_derivative output is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e51760",
   "metadata": {},
   "source": [
    "#### Exercise 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d977d1",
   "metadata": {},
   "source": [
    "Create back propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def back_propagation(y, a, z, W, dW, db, act_deri):\n",
    "    '''\n",
    "    Backprop step. Note that derivative of multinomial cross entropy\n",
    "    loss is the same as that of binary cross entropy loss. See\n",
    "    https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "    for a nice derivation.\n",
    "    '''\n",
    "    L = len(W)-1\n",
    "    \n",
    "    # y_hat - y\n",
    "    # delta[L] = None\n",
    "    for l in range(L,0,-1):\n",
    "        # db = delta(l)\n",
    "        db[l] = None\n",
    "        \n",
    "        # dW = a(l-1) * delta(l)\n",
    "        dW[l] = None\n",
    "        \n",
    "        if l > 1:\n",
    "            # recalculate delta in backward layer\n",
    "            # dAct_func(z(l-1)) * (W(l) * delta(l))\n",
    "            delta[l-1] = None\n",
    "            \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381eb83",
   "metadata": {},
   "source": [
    "Create activation derivative variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_deri = [None, ReLu_derivative, Sigmoid_derivative, Softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "y_this = y_train[0,:]\n",
    "\n",
    "a, z, delta, dW, db = forward_one_step(x_this, W, B, act_funcs)\n",
    "dW, db = back_propagation(y_this, a, z, W, dW, db, act_deri)\n",
    "\n",
    "lenW = [0, 64, 6, 5]\n",
    "for i in range(4):\n",
    "    assert len(dW[i]) == lenW[i]\n",
    "    \n",
    "print(\"dW\", dW)\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0228326",
   "metadata": {},
   "source": [
    "## 5. Parameters Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ceab64",
   "metadata": {},
   "source": [
    "In the training, to improve accuracy, you need to update weight/bias while training.\n",
    "Weight and bias update equations are  \n",
    "\n",
    "$$\n",
    "W_{new}^{(i)} = W_{old}^{(i)} - \\alpha * \\delta W\n",
    "$$  \n",
    "\n",
    "$$\n",
    "B_{new}^{(i)} = B_{old}^{(i)} - \\alpha * \\delta B\n",
    "$$\n",
    "\n",
    "When $\\alpha$ is learning rate. and $i$ is the layer number of network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96b408",
   "metadata": {},
   "source": [
    "#### Exercise 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bbb06",
   "metadata": {},
   "source": [
    "Create <code>update_step</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a941ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "def update_step(W, B, dW, db, alpha):\n",
    "    L = len(W)-1\n",
    "    for l in range(1,L+1):\n",
    "        # W[l] = None\n",
    "        # B[l] = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "x_this = X_train[0,:].T\n",
    "y_this = y_train[0,:]\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "a, z, delta, dW, db = forward_one_step(x_this, W, B, act_funcs)\n",
    "dW, db = back_propagation(y_this, a, z, W, dW, db, act_deri)\n",
    "\n",
    "W_new, B_new = update_step(W, B, dW, db, alpha)\n",
    "W_new_2, B_new_2 = update_step(W_new, B_new, dW, db, alpha)\n",
    "\n",
    "result_w = np.array_equal(W, W_new)\n",
    "result_w2 = np.array_equal(W_new, W_new_2)\n",
    "assert W[2].shape == W_new[2].shape and W[1].shape == W_new_2[1].shape, \"W_new shape must be the same\"\n",
    "assert not result_w and not result_w2, \"Weight must be updated\"\n",
    "\n",
    "result_b = np.array_equal(B, B_new)\n",
    "result_b2 = np.array_equal(B_new, B_new_2)\n",
    "assert B[3].shape == B_new[3].shape and B[1].shape == B_new_2[1].shape, \"b_new shape must be the same\"\n",
    "assert not result_b and not result_b2, \"Bias must be updated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86f200",
   "metadata": {},
   "source": [
    "## Put it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68844b20",
   "metadata": {},
   "source": [
    "#### Exercise 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c3a2d",
   "metadata": {},
   "source": [
    "Create training code using the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5319f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade cell - do not remove\n",
    "\n",
    "cost_arr = [] \n",
    "\n",
    "alpha = 0.01\n",
    "max_iter = 100\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    # random index of m_train\n",
    "    order = np.random.permutation(m_train)\n",
    "    for i in range(0, m_train):\n",
    "        # Grab the pattern order[i]\n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "        \n",
    "        # Feed forward step\n",
    "        a, z, delta, dW, db = None, None, None, None, None\n",
    "        # calulate loss for each epoch\n",
    "        loss_this_pattern = 0 #(calculate loss here)\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        # back propagation\n",
    "        dW, db = None, None\n",
    "        # update weight, bias (1 line)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    cost_arr.append(loss_this_iter[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79677bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function - do not remove\n",
    "\n",
    "for i in range(max_iter):\n",
    "    print('Epoch %d train loss %f' % (i + 1, cost_arr[i]))\n",
    "assert len(cost_arr) == max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a3288",
   "metadata": {},
   "source": [
    "### Take Home Exercise\n",
    "\n",
    "1. Plot the loss value into graph using pyplot\n",
    "2. Create Prediction function to predict the test set which we have separated from above. Calculate the accuracy of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1dca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
